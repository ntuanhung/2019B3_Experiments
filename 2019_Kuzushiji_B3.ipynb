{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019_Kuzushiji_B3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntuanhung/2019B3_Experiments/blob/master/2019_Kuzushiji_B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epBFs0e9LIE2",
        "colab_type": "text"
      },
      "source": [
        "# For 3rd year student experiment\n",
        "Kuzushiji recognition is a challenging task in image classification field. The reasons are:\n",
        "+ Around 4,000 classes.\n",
        "+ A large variance in many classes.\n",
        "+ Images are degraded and have different solutions.\n",
        "+ Great imbalance among the classes in database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AZn2oEh-rfZ",
        "colab_type": "text"
      },
      "source": [
        "# October, 2019\n",
        "\n",
        "Try multiple Convolutional Neural Networks (CNNs) to recognize the Japanese historical character images.\n",
        "  + AlexNet\n",
        "  + VGGNet (VGG16 or VGG19)\n",
        "  + ResNet (Residual Network-34, -50, -101, or -152)\n",
        "  + InceptionNet (v1, v2, v3, or v4)\n",
        "\n",
        "Objectives:\n",
        "  + Learn about how to train network with our data and reproduce the results on KMNIST datasets. (implementation)\n",
        "  + Visualize the training process (plot train and test error and loss during training) as well as the results (plot misrecognized images).\n",
        "  + Understand theory of neural network and optimization process.\n",
        "  + Try data augmentation.\n",
        "  + Visualize the feature maps.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XF7sHG_6vjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "cellView": "both",
        "outputId": "bcb41e30-bcb4-4915-ef0a-0dc3e0e4d063"
      },
      "source": [
        "#@title Mount Google Drive { vertical-output: true }\n",
        "## Copy files from drive to local of runtime\n",
        "%cd /content\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p kmnist\n",
        "%cd /content/kmnist/\n",
        "\n",
        "#!cp /content/drive/My\\ Drive/ojc/kmnist/*.* /content/kmnist\n",
        "\n",
        "## Create symbolic links to Google Drive\n",
        "!ln -s /content/drive/My\\ Drive/ojc/kmnist/src /content/kmnist\n",
        "!ln -s /content/drive/My\\ Drive/ojc/kmnist/log /content/kmnist\n",
        "!ln -s /content/drive/My\\ Drive/ojc/kmnist/model /content/kmnist\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/kmnist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOW-kAyQ99nD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "cellView": "both",
        "outputId": "a6b14d06-d123-4ff4-ed76-62ee06e3d06d"
      },
      "source": [
        "#@title Download data { vertical-output: true }\n",
        "dataset_name = \"kanji\" #@param [\"k10\", \"k49\", \"kanji\", \"other\"]\n",
        "\n",
        "## Execute script to download database \n",
        "!python ./src/download_data.py --dataset $dataset_name\n",
        "\n",
        "## and verify the result\n",
        "!ls\n",
        "\n",
        "## Extract kkanji.tar file\n",
        "if dataset_name == \"kanji\":\n",
        "  !tar -xf kkanji.tar\n",
        "  !ls ./kkanji2/ -1 | wc -l"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset kanji ...\n",
            "Downloading kkanji.tar - 316.7 MB\n",
            "100% 316690/316691 [00:47<00:00, 6611.65KB/s]\n",
            "All dataset files downloaded!\n",
            "kkanji.tar  log  model\tsrc\n",
            "3832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTJ3PMeqPMUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "cellView": "form",
        "outputId": "da1e34fc-b501-4ceb-abae-1a6dfa916c28"
      },
      "source": [
        "#@title Prepare training/testing sets for Kanji { vertical-output: true, output-height: 20 }\n",
        "import os\n",
        "import random\n",
        "folderPath = \"./kkanji2/\"\n",
        "kkanji2_train_fout = open(\"kkanji2_train.txt\", \"wt\")\n",
        "kkanji2_test_fout = open(\"kkanji2_test.txt\", \"wt\")\n",
        "labelList = []\n",
        "countOnefile = 0\n",
        "countTrainFiles = 0\n",
        "countTestFiles = 0\n",
        "\n",
        "if os.path.exists(folderPath) and os.path.isdir(folderPath):\n",
        "  for subfolder in os.listdir(folderPath):\n",
        "    if os.path.isdir(folderPath + subfolder):\n",
        "      fileList = []\n",
        "      for filename in os.listdir(folderPath + subfolder):\n",
        "        if filename.endswith(\".png\"):\n",
        "          fileList.append(filename)\n",
        "      assert(len(fileList) > 0)\n",
        "      if len(fileList) == 1:\n",
        "        countOnefile+=1\n",
        "\n",
        "      labelList.append(subfolder)\n",
        "      random.shuffle(fileList)\n",
        "      numTest = max(1, int(0.2 * len(fileList)))\n",
        "      for idx in range(0, numTest, 1):\n",
        "        kkanji2_test_fout.write(\"%s\\n\"%(folderPath + subfolder + \"/\" + fileList[idx]))\n",
        "        countTestFiles += 1\n",
        "\n",
        "      for idx in range(numTest, len(fileList), 1):\n",
        "        kkanji2_train_fout.write(\"%s\\n\"%(folderPath + subfolder + \"/\" + fileList[idx]))\n",
        "        countTrainFiles += 1\n",
        "\n",
        "kkanji2_train_fout.close()\n",
        "kkanji2_test_fout.close()\n",
        "\n",
        "sorted(labelList)\n",
        "with open(\"kkanji2_label.txt\", \"wt\") as label_fout:\n",
        "  for label in labelList:\n",
        "    label_fout.write(\"%s\\n\"%(label))\n",
        "\n",
        "print(\"Total %d classes in Kanji dataset.\"%(len(labelList)))\n",
        "print(\"Number of classes without training images = %d\"%(countOnefile))\n",
        "print(\"%d training images and %d testing images.\"%(countTrainFiles, countTestFiles))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 3832 classes in Kanji dataset.\n",
            "Number of classes without training images = 815\n",
            "112071 training images and 28353 testing images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to9E5PZXnXH5",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Try datagenerator { vertical-output: true }\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import argparse\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
        "\n",
        "## Constants\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 128\n",
        "MAX_EPOCHES = 20\n",
        "IMG_HEIGHT, IMG_WIDTH = 64, 64\n",
        "NUM_CHANNELS = 1\n",
        "LOG_PATH = \"./tensorlog/\"\n",
        "AUGMENTATION_PROB = 0.0\n",
        "\n",
        "labelList = []\n",
        "with open(\"kkanji2_label.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    if line.strip()!=\"\":\n",
        "      labelList.append(line.strip())\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, '/')\n",
        "  # The second to last is the class-directory\n",
        "  return parts[-2] == labelList\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=NUM_CHANNELS)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "def process_path(filePath):\n",
        "  label = get_label(filePath)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(filePath)\n",
        "  img = decode_img(img)\n",
        "  return img, label\n",
        "\n",
        "def prepare_dataset(fileTxt, cache=True, shuffle_buffer_size=1000):\n",
        "  fileList = []\n",
        "  with open(fileTxt) as f:\n",
        "    for line in f.readlines():\n",
        "      if line.strip()!=\"\":\n",
        "        fileList.append(line.strip())\n",
        "  shuffle(fileList) \n",
        "  # Parse the list of file path\n",
        "  data_list = tf.data.Dataset.from_tensor_slices(fileList)\n",
        "  ds = data_list.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # This is a small dataset, only load it once, and keep it in memory.\n",
        "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "  # fit in memory.\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  if shuffle_buffer_size>0:\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return fileList, ds\n",
        "\n",
        "## Simple model\n",
        "def get_uncompiled_model(input_shape, num_classes):\n",
        "  inputs = tf.keras.Input(shape=input_shape, name='digits')\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding=\"same\",\n",
        "             input_shape=input_shape, name=\"conv1\")(inputs)\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv2\")(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv3\")(x)\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv4\")(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation='relu', name=\"fc10\")(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  outputs = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "def get_compiled_model(input_shape, num_classes, networkName=\"\", optimizerName=\"\"):\n",
        "  model = get_uncompiled_model(input_shape, num_classes)\n",
        "  # Build model with optimizer\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adadelta(),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  parser = argparse.ArgumentParser(description='kuzushiji with different datasets')\n",
        "  parser.add_argument('--dataset', dest='dataset', \n",
        "                      default=\"kanji\",\n",
        "                      help='dataset name should be k10/k49/kanji')\n",
        "  parser.add_argument('--model', dest='model', \n",
        "                      default=\"simpleNet\",\n",
        "                      help='model name should be simpleNet/AlexNet/VGGNet/ResNet/InceptionNet')\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  networkName = args.model # \"simpleNet\" \"AlexNet\" \"VGGNet\" \"ResNet\" \"InceptionNet\"\n",
        "  optimizerName = \"ADEL\" #\"ADEL\" \"SGD\" \"ADAM\" \"RMSP\"\n",
        "  datasetName = args.dataset\n",
        "  logFile = \"_\".join([networkName, optimizerName, datasetName]) + \"_\" + datetime.now().strftime('%Y.%m.%d_%H.%M.%S')\n",
        "  logFolder = LOG_PATH + logFile + \"/\"\n",
        "\n",
        "  # prepare data generator\n",
        "  trainFileList, train_ds = prepare_dataset(\"kkanji2_train.txt\", cache=False,\n",
        "                                            shuffle_buffer_size=len(labelList))\n",
        "  testFileList, test_ds = prepare_dataset(\"kkanji2_test.txt\", cache=False,\n",
        "                                          shuffle_buffer_size=-1)\n",
        "  num_classes = len(labelList)\n",
        "  print(\"Total %d categories. Train set with %d images and test set with %d images.\"\n",
        "        %(num_classes, len(trainFileList), len(testFileList)))\n",
        "\n",
        "  if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "      input_shape = (NUM_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
        "  else:\n",
        "      input_shape = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
        "  model = get_compiled_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "\n",
        "  ## Callback functions --> use for multiple purposes\n",
        "  # save the best model based on validation loss\n",
        "  fpath = logFolder + 'weights.{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5'\n",
        "  cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=fpath, monitor='val_loss', \n",
        "                                             verbose=1, save_best_only=True, mode='auto')\n",
        "  # early stopping training process \n",
        "  es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, \n",
        "                                           verbose=1, mode='auto')\n",
        "  \n",
        "  # save the log information into log file\n",
        "  lg_cb = tf.keras.callbacks.CSVLogger(logFolder + logFile + '.csv')\n",
        "  \n",
        "  # Train and evaluate model on dataset using generator instead of whole dataset\n",
        "  print(\"Training %s\"%logFile)\n",
        "  model.fit_generator(generator = train_ds, validation_data = test_ds,\n",
        "                      steps_per_epoch = int(len(trainFileList)/BATCH_SIZE),\n",
        "                      validation_steps= int(len(testFileList)/BATCH_SIZE),\n",
        "                      callbacks = [cp_cb, es_cb, tb_cb, lg_cb],\n",
        "                      epochs=MAX_EPOCHES, verbose=1)\n",
        "  \n",
        "  print(\"Evaluating %s\"%logFile)\n",
        "  train_score = model.evaluate_generator(generator=train_ds, verbose=0,\n",
        "                                         steps = int(len(trainFileList)/BATCH_SIZE)) \n",
        "  test_score = model.evaluate_generator(generator=test_ds, verbose=0,\n",
        "                                        steps = int(len(testFileList)/BATCH_SIZE))\n",
        "  print('Train loss:', train_score[0])\n",
        "  print('Train accuracy:', train_score[1])\n",
        "  print('Test loss:', test_score[0])\n",
        "  print('Test accuracy:', test_score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xF4E5L4-jT1",
        "colab_type": "code",
        "outputId": "9fe0a343-6556-47ca-f175-781d18c2bc20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "cellView": "both"
      },
      "source": [
        "#@title Preliminary experiments on KMNIST  { output-height: 10 }\n",
        "dataset_name = \"kanji\" #@param [\"k10\", \"k49\", \"kanji\"]\n",
        "model_name = \"simpleNet\" #@param [\"simpleNet\", \"AlexNet\", \"VGGNet\", \"ResNet\", \"InceptionNet\", \"kNN\"]\n",
        "if model_name == \"kNN\":\n",
        "  ## Example of KNN method\n",
        "  !python ./src/kuzushiji_knn.py --dataset $dataset_name\n",
        "else:\n",
        "  ## Training an example of CNN\n",
        "  if dataset_name == \"kanji\":\n",
        "    !python ./src/kuzushiji_cnn_kanji.py --model $model_name\n",
        "  else:\n",
        "    !python ./src/kuzushiji_cnn.py --dataset $dataset_name --model $model_name"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=-1, n_neighbors=4, p=2,\n",
            "                     weights='distance')\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMbHZl4nU9bE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "cellView": "both",
        "outputId": "6b969431-3f1c-45c8-acad-a9353b85d8ab"
      },
      "source": [
        "#@title Data generator by tensorflow { vertical-output: true }\n",
        "# change directory to kmnist project\n",
        "%cd /content/kmnist/\n",
        "# make sure use tensorflow 2.0\n",
        "%tensorflow_version 2.x \n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
        "\n",
        "## Constants\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 128\n",
        "MAX_EPOCHES = 20\n",
        "IMG_HEIGHT, IMG_WIDTH = 64, 64\n",
        "NUM_CHANNELS = 1\n",
        "LOG_PATH = \"./tensorlog/\"\n",
        "AUGMENTATION_PROB = 0.0\n",
        "\n",
        "labelList = []\n",
        "with open(\"kkanji2_label.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    if line.strip()!=\"\":\n",
        "      labelList.append(line.strip())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_images(dataset, n_images, samples_per_image):\n",
        "    plt.figure() #figsize=(10.0, 10.0))\n",
        "    k=0\n",
        "    for images, labels in dataset.repeat(samples_per_image).batch(n_images):\n",
        "      label_np = labels.numpy()\n",
        "      pos = np.argmax(label_np, axis=-1)\n",
        "      s = \" \".join([ labelList[idx] for idx in pos ])\n",
        "      print(images.shape, labels.shape, s)\n",
        "      for n in range(k, k+n_images):\n",
        "        ax = plt.subplot(samples_per_image,n_images,n+1)\n",
        "        plt.imshow(np.squeeze(images[n-k]), cmap = plt.get_cmap(\"gray\"))\n",
        "        plt.title(labelList[pos[n-k]])\n",
        "        plt.axis('off')\n",
        "      k+=n_images\n",
        "      if k>=samples_per_image*n_images:\n",
        "        break\n",
        "    plt.show()\n",
        "\n",
        "def rotate(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Rotation augmentation\n",
        "  Args: x: Image\n",
        "  Returns: Augmented image\n",
        "  \"\"\"\n",
        "  # Rotate 0, 90, 180, 270 degrees\n",
        "  return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)), y\n",
        "\n",
        "def flip(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Flip augmentation\n",
        "    Args: x: Image to flip\n",
        "    Returns: Augmented image\n",
        "    \"\"\"\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_flip_up_down(x)\n",
        "    return x, y\n",
        "\n",
        "def color(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Color augmentation\n",
        "    Args: x: Image\n",
        "    Returns: Augmented image\n",
        "    \"\"\"\n",
        "    x = tf.image.random_hue(x, 0.08)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
        "    x = tf.image.random_brightness(x, 0.05)\n",
        "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
        "    return x, y\n",
        "\n",
        "def zoom(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Zoom augmentation\n",
        "    Args: x: Image\n",
        "    Returns: Augmented image\n",
        "    \"\"\"\n",
        "    # Generate 20 crop settings, ranging from a 1% to 20% crop.\n",
        "    scales = list(np.arange(0.8, 1.0, 0.01))\n",
        "    boxes = np.zeros((len(scales), 4))\n",
        "    for i, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - (0.5 * scale)\n",
        "        x2 = y2 = 0.5 + (0.5 * scale)\n",
        "        boxes[i] = [x1, y1, x2, y2]\n",
        "    def random_crop(img):\n",
        "        # Create different crops for an image\n",
        "        crops = tf.image.crop_and_resize([img], boxes=boxes, \n",
        "                                         box_indices=np.zeros(len(scales)), \n",
        "                                         crop_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "        # Return a random crop\n",
        "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "    # Only apply cropping 50% of the time\n",
        "    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x)), y\n",
        "\n",
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, '/')\n",
        "  # The second to last is the class-directory\n",
        "  return parts[-2] == labelList\n",
        "\n",
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=NUM_CHANNELS)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "\n",
        "def process_path(filePath):\n",
        "  label = get_label(filePath)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(filePath)\n",
        "  img = decode_img(img)\n",
        "  return img, label\n",
        "\n",
        "def prepare_dataset(fileTxt, cache=True, shuffle_buffer_size=1000):\n",
        "  fileList = []\n",
        "  with open(fileTxt) as f:\n",
        "    for line in f.readlines():\n",
        "      if line.strip()!=\"\":\n",
        "        fileList.append(line.strip())\n",
        "  shuffle(fileList)\n",
        "  data_list = tf.data.Dataset.from_tensor_slices(fileList)\n",
        "  ds = data_list.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # if preproc_fn.keywords is not None and 'resize' not in preproc_fn.keywords:\n",
        "  #   assert batch_size == 1, \"Batching images must be of the same size\"\n",
        "  # ds = ds.map(preproc_fn, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # augmentations\n",
        "  augmentations = [rotate, zoom]\n",
        "  # Add the augmentations to the dataset\n",
        "  for f in augmentations:\n",
        "      # Apply the augmentation, run 4 jobs in parallel.\n",
        "      # ds = ds.map(f, num_parallel_calls=4)\n",
        "      ds = ds.map(lambda x, y: tf.cond(tf.random.uniform([], 0, 1) > AUGMENTATION_PROB, \n",
        "                                    lambda: f(x, y), lambda: (x,y)), num_parallel_calls=AUTOTUNE)\n",
        "  # Make sure that the values are still in [0, 1]\n",
        "  ds = ds.map(lambda x, y: (tf.clip_by_value(x, 0.0, 1.0), y), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # This is a small dataset, only load it once, and keep it in memory.\n",
        "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "  # fit in memory.\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  if shuffle_buffer_size>0:\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  return fileList, ds\n",
        "\n",
        "## Simple model\n",
        "def get_uncompiled_model(input_shape, num_classes):\n",
        "  inputs = tf.keras.Input(shape=input_shape, name='digits')\n",
        "  x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding=\"same\",\n",
        "             input_shape=input_shape, name=\"conv1\")(inputs)\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv2\")(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv3\")(x)\n",
        "  x = Conv2D(64, (3, 3), activation='relu', padding=\"same\", name=\"conv4\")(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # x = Conv2D(128, (3, 3), activation='relu', padding=\"same\", name=\"conv5\")(x)\n",
        "  # x = Conv2D(128, (3, 3), activation='relu', padding=\"same\", name=\"conv6\")(x)\n",
        "  # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  # x = Conv2D(128, (3, 3), activation='relu', padding=\"same\", name=\"conv7\")(x)\n",
        "  # x = Conv2D(128, (3, 3), activation='relu', padding=\"same\", name=\"conv8\")(x)\n",
        "  # x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Flatten()(x)\n",
        "  # x = Dense(1024, activation='relu', name=\"fc9\")(x)\n",
        "  # x = Dropout(0.25)(x)\n",
        "  x = Dense(128, activation='relu', name=\"fc10\")(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "  outputs = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "def get_compiled_model(input_shape, num_classes, networkName=\"\", optimizerName=\"\"):\n",
        "  model = get_uncompiled_model(input_shape, num_classes)\n",
        "  # Build model with optimizer\n",
        "  # model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "  #               optimizer=tf.keras.optimizers.Adadelta(),\n",
        "  #               metrics=['accuracy'])\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  networkName = \"simpleCNN\" # \"simpleCNN\" \"AlexNet\" \"VGGNet\" \"ResNet\" \"InceptionNet\"\n",
        "  optimizerName = \"RMSP\" #\"ADEL\" \"SGD\" \"ADAM\" \"RMSP\"\n",
        "  datasetName = \"kanji\"\n",
        "  logFile = \"_\".join([networkName, optimizerName, datasetName]) + \"_\" + datetime.now().strftime('%Y.%m.%d_%H.%M.%S')\n",
        "  logFolder = LOG_PATH + logFile + \"/\"\n",
        "\n",
        "  # prepare data generator\n",
        "  trainFileList, train_ds = prepare_dataset(\"kkanji2_train.txt\", cache=False,\n",
        "                                            shuffle_buffer_size=len(labelList))\n",
        "  testFileList, test_ds = prepare_dataset(\"kkanji2_test.txt\", cache=False,\n",
        "                                          shuffle_buffer_size=-1)\n",
        "  num_classes = len(labelList)\n",
        "  print(\"Total %d categories. Train set with %d images and test set with %d images.\"\n",
        "        %(num_classes, len(trainFileList), len(testFileList)))\n",
        "  # for img, lb in train_ds:\n",
        "  #    print(img.shape, np.argmax(lb.numpy(), axis=-1), np.min(img.numpy()), np.max(img.numpy()))  \n",
        "  # plot_images(train_ds, n_images=8, samples_per_image=10)\n",
        "\n",
        "  if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "      input_shape = (NUM_CHANNELS, IMG_HEIGHT, IMG_WIDTH)\n",
        "  else:\n",
        "      input_shape = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
        "  model = get_compiled_model(input_shape, num_classes)\n",
        "  model.summary()\n",
        "\n",
        "  # if args.resume !=\"\":\n",
        "  #   model.load_weights(args.resume)\n",
        "  #   print(\"Succesfully loaded weights from %s\"%(args.resume))\n",
        "\n",
        "  # Callback functions\n",
        "  fpath = logFolder + 'weights.{epoch:02d}-{loss:.2f}-{accuracy:.2f}-{val_loss:.2f}-{val_accuracy:.2f}.hdf5'\n",
        "  cp_cb = tf.keras.callbacks.ModelCheckpoint(filepath=fpath, monitor='val_loss', \n",
        "                                             verbose=1, save_best_only=True, mode='auto')\n",
        "  es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, \n",
        "                                           verbose=1, mode='auto')\n",
        "  tb_cb = tf.keras.callbacks.TensorBoard(log_dir=logFolder, histogram_freq=1)\n",
        "  lg_cb = tf.keras.callbacks.CSVLogger(logFolder + logFile + '.csv')\n",
        "  \n",
        "  # Train and evaluate model on dataset\n",
        "  print(\"Training %s\"%logFile)\n",
        "  model.fit_generator(generator = train_ds, validation_data = test_ds,\n",
        "                      steps_per_epoch = int(len(trainFileList)/BATCH_SIZE),\n",
        "                      validation_steps= int(len(testFileList)/BATCH_SIZE),\n",
        "                      callbacks = [cp_cb, es_cb, tb_cb, lg_cb],\n",
        "                      epochs=MAX_EPOCHES, verbose=1)\n",
        "                      \n",
        "  json_string = model.to_json()\n",
        "  open(os.path.join(logFolder + logFile + '.json'), 'w').write(json_string)\n",
        "  \n",
        "  print(\"Evaluating %s\"%logFile)\n",
        "  train_score = model.evaluate_generator(generator=train_ds, verbose=0,\n",
        "                                         steps = int(len(trainFileList)/BATCH_SIZE)) \n",
        "  test_score = model.evaluate_generator(generator=test_ds, verbose=0,\n",
        "                                        steps = int(len(testFileList)/BATCH_SIZE))\n",
        "  print('Train loss:', train_score[0])\n",
        "  print('Train accuracy:', train_score[1])\n",
        "  print('Test loss:', test_score[0])\n",
        "  print('Test accuracy:', test_score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kmnist\n",
            "TensorFlow 2.x selected.\n",
            "Total 3832 categories. Train set with 112071 images and test set with 28353 images.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "digits (InputLayer)          [(None, 64, 64, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 64, 64, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "fc10 (Dense)                 (None, 128)               2097280   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 3832)              494328    \n",
            "=================================================================\n",
            "Total params: 2,684,280\n",
            "Trainable params: 2,684,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training simpleCNN_RMSP_kanji_2019.11.02_00.41.22\n",
            "Epoch 1/20\n",
            "874/875 [============================>.] - ETA: 0s - loss: 6.2712 - accuracy: 0.0676\n",
            "Epoch 00001: val_loss improved from inf to 5.69159, saving model to ./tensorlog/simpleCNN_RMSP_kanji_2019.11.02_00.41.22/weights.01-6.27-0.07-5.69-0.15.hdf5\n",
            "875/875 [==============================] - 278s 318ms/step - loss: 6.2702 - accuracy: 0.0677 - val_loss: 5.6916 - val_accuracy: 0.1472\n",
            "Epoch 2/20\n",
            "874/875 [============================>.] - ETA: 0s - loss: 5.4703 - accuracy: 0.1562\n",
            "Epoch 00002: val_loss improved from 5.69159 to 5.17518, saving model to ./tensorlog/simpleCNN_RMSP_kanji_2019.11.02_00.41.22/weights.02-5.47-0.16-5.18-0.24.hdf5\n",
            "875/875 [==============================] - 270s 309ms/step - loss: 5.4703 - accuracy: 0.1562 - val_loss: 5.1752 - val_accuracy: 0.2391\n",
            "Epoch 3/20\n",
            "874/875 [============================>.] - ETA: 0s - loss: 5.1485 - accuracy: 0.2017\n",
            "Epoch 00003: val_loss improved from 5.17518 to 4.91291, saving model to ./tensorlog/simpleCNN_RMSP_kanji_2019.11.02_00.41.22/weights.03-5.15-0.20-4.91-0.28.hdf5\n",
            "875/875 [==============================] - 268s 306ms/step - loss: 5.1481 - accuracy: 0.2017 - val_loss: 4.9129 - val_accuracy: 0.2823\n",
            "Epoch 4/20\n",
            "874/875 [============================>.] - ETA: 0s - loss: 4.9784 - accuracy: 0.2279\n",
            "Epoch 00004: val_loss improved from 4.91291 to 4.81935, saving model to ./tensorlog/simpleCNN_RMSP_kanji_2019.11.02_00.41.22/weights.04-4.98-0.23-4.82-0.31.hdf5\n",
            "875/875 [==============================] - 267s 305ms/step - loss: 4.9783 - accuracy: 0.2279 - val_loss: 4.8193 - val_accuracy: 0.3083\n",
            "Epoch 5/20\n",
            "573/875 [==================>...........] - ETA: 1:13 - loss: 4.9056 - accuracy: 0.2376"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwDQkJ-ZsDIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRou2OePNvnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "cellView": "both",
        "outputId": "3f5bfdb0-184d-462a-8ce7-548e8de7e3b5"
      },
      "source": [
        "#@title Visualize training process by tensorboard\n",
        "LOG_PATH = \"/content/kmnist/tensorlog/simpleCNN_RMSP_kanji_2019.11.02_00.41.22\"\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_PATH)\n",
        ")\n",
        "# Install\n",
        "! npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 6006 (TensorBoard assumed running)\n",
        "get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')\n",
        "\n",
        "# Get url\n",
        "! cat url.txt"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.0\n",
            "added 35 packages from 21 contributors in 2.497s\n",
            "your url is: https://tall-skunk-50.localtunnel.me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBLVRgxONePW",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title Augmentation experiments\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1-4uEL2NobA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Visualize feature maps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ-LlikyBGCW",
        "colab_type": "text"
      },
      "source": [
        "### Experiment results\n",
        "KMNIST with only 10 classes is really easy to work even with KNN method.\n",
        "\n",
        "From now, let try with Hiragana classes.\n",
        "\n",
        "### Discussions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42w98OWkJcF2",
        "colab_type": "text"
      },
      "source": [
        "# November, 2019"
      ]
    }
  ]
}